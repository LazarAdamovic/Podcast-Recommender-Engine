{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Recommender Engine\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-proccessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "import pandas as pd \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataframe\n",
    "data=pd.read_csv('Data/data_cleaned.csv')\n",
    "\n",
    "#create copy\n",
    "df=data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rid of first column since its not needed\n",
    "\n",
    "df = df.iloc[: , 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# de-dummies variables\n",
    "# all the category variables were turned into dummy-variables in order to aggregate the duplicate reviews\n",
    "# they now need to be turned back into categorical variables\n",
    "\n",
    "#create new \n",
    "df['tags']=''\n",
    "\n",
    "# takes dummmies and puts them in df.tags\n",
    "for col_name in df.columns[7:]:\n",
    "    df.loc[df[col_name]==1,'tags']= df['tags']+' '+col_name\n",
    "\n",
    "# get rid of columns with dummy variables\n",
    "df=df[['podcast_id', 'author_id', 'created_at', 'podcast', 'title', 'rating', #gets rid of dummie features\n",
    "       'content','tags']]\n",
    "\n",
    "\n",
    "# slice strings to get rid of \"category_\" part of the value\n",
    "\n",
    "df['tags']=df['tags'].astype(str)\n",
    "\n",
    "df['tags']=df['tags'].apply(lambda x: [w[9:] for w in x.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change date+time value in in the 'created_at' column into 'year'\n",
    "\n",
    "# change to datetime\n",
    "df['created_at']=pd.to_datetime(df['created_at'])\n",
    "\n",
    "# change datetime to year\n",
    "df['year']=df['created_at'].apply(lambda x: x.strftime('%Y'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn rating into string \n",
    "df['rating'] = df.rating.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-processsing content\n",
    "\n",
    "#installing all packages\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# pseudo-pipeline\n",
    "def cleaning_content(x):\n",
    "    \n",
    "    # tokenize and remove punctuation\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokenized=tokenizer.tokenize(x)\n",
    "    \n",
    "    # turn all characters into lowercase\n",
    "    lowered = [w.lower() for w in tokenized]\n",
    "    \n",
    "    #removing stopwords\n",
    "    stop_words = stopwords.words('english')\n",
    "    rm_stopwords=[w for w in lowered if not w in stop_words]\n",
    "    \n",
    "    #lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()  \n",
    "    lemmatized=[lemmatizer.lemmatize(token) for token in rm_stopwords]\n",
    "\n",
    "    return lemmatized\n",
    "\n",
    "df['lemmatized']=[cleaning_content(x) for x in df['content']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only necessary rows\n",
    "df=df[['podcast_id','author_id','podcast','year','rating','lemmatized','tags']]\n",
    "\n",
    "# turn rating into string \n",
    "df['rating'] = df.rating.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a soup column which combines all the features\n",
    "# since the dataframe has some columns which are strings and others which are lists of strings, addditional processing needs to be performed\n",
    "\n",
    "# combining all the stringed columns into a column which will be called 'soup'\n",
    "df['string_soup']=df['author_id']+' '+df['year']+' '+df['podcast']+' '+df['rating']\n",
    "\n",
    "# this soup column will not have the rating included. Later will be testing whether changes in parameters will create better results\n",
    "df['string_soup_r']=df['author_id']+' '+df['year']+' '+df['podcast']\n",
    "\n",
    "\n",
    "# combine all lists \n",
    "def extender(x):\n",
    "    return ' '.join(x['tags'] + x['lemmatized'])\n",
    "df['lst_soup']=df.apply(extender, axis=1)\n",
    "\n",
    "\n",
    "# combine both columns into one soup\n",
    "def extender_soup(x):\n",
    "    return x['string_soup'] + ' ' + x['lst_soup']\n",
    "df['soup']=df.apply(extender_soup, axis=1)\n",
    "\n",
    "# combine both columns into one soup wihout rating\n",
    "def extender_soup(x):\n",
    "    return x['string_soup_r'] + ' ' + x['lst_soup']\n",
    "df['soup_r']=df.apply(extender_soup, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making copy of dataframe\n",
    "\n",
    "df_soup=df.copy()\n",
    "df_soup_r=df.copy() #this will be the new dataframe with rating as it's own column\n",
    "\n",
    "# dropping all columns except for the soup\n",
    "\n",
    "df_soup=df_soup[['podcast_id','soup']] \n",
    "df_soup_r=df_soup_r[['podcast_id','soup','rating']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_soup_r.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_soup.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import CountVectorizer and create the count matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Run Vectorizer\n",
    "count = CountVectorizer(max_features=5000)\n",
    "count_matrix = count.fit_transform(df_soup['soup'])\n",
    "count_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#turn matrix into dataframe\n",
    "df_countmatrix=pd.DataFrame(count_matrix, columns=['vectors'])\n",
    "\n",
    "#merge podcast_id and countmatrix\n",
    "df_countmatrix=df_soup[['podcast_id']].join(df_countmatrix)\n",
    "df_countmatrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def recommend_podcast_cosine(df, index):\n",
    "    smin = None\n",
    "    imax2 = None\n",
    "    row = df.iloc[index]\n",
    "    \n",
    "    # loop through dataframe rows\n",
    "    for index2, row2 in df.iterrows():\n",
    "    \n",
    "        #loop through column in those rows\n",
    "        #index != index2:\n",
    "        if row['podcast_id'] != row2['podcast_id']: #index != index2:\n",
    "            s = cosine_similarity(row['vectors'], row2['vectors'])\n",
    "            \n",
    "            # find smallest distance between two vectors\n",
    "            if smin == None or s < smin:\n",
    "                smin = s\n",
    "                imax2 = index2\n",
    "    return imax2, smin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1234\n",
    "imax2 = recommend_podcast_cosine(df_countmatrix,i)\n",
    "x=imax2[0]\n",
    "print(list(df_soup.iloc[i]))\n",
    "print(list(df_soup.iloc[x]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jaccard Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jaccard Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create jaccard simualirty score\n",
    "# there is no prebuilt jaccard function that takes both float points and strings, so I created my own\n",
    "\n",
    "def jaccard_similarity(a, b): \n",
    "    # convert to set and tokenize soup\n",
    "    a1 = set(a.split()) ####  can remove this and tokenize df_soup['soup']\n",
    "    b1 = set(b.split())\n",
    "    \n",
    "    # calucate jaccard similarity\n",
    "    j = float(len(a1.intersection(b1))) / len(a1.union(b1))\n",
    "    return j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return recommended Podcast ID for input index of a review\n",
    "def recommend_podcast_jaccard(df, index):\n",
    "    smax = 0\n",
    "    row = df.iloc[index]\n",
    "    for index2, row2 in df.iterrows():\n",
    "    # print(index, row['podcast_id'], row['soup'])\n",
    "    \n",
    "    #loop through column in those rows and get the cosine similarity \n",
    "        #index != index2:\n",
    "        if row['podcast_id'] != row2['podcast_id']: #index != index2:\n",
    "            s = jaccard_similarity(row['soup'], row2['soup'])\n",
    "            # print(index, index2, row['podcast_id'], row['podcast_id'], s)\n",
    "            \n",
    "            #find greates jaccard scores\n",
    "            if s > smax:\n",
    "                smax = s\n",
    "                imax2 = index2\n",
    "    return imax2, smax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results\n",
    "\n",
    "i = 12345\n",
    "imax2 = recommend_podcast_jaccard(df_soup, i)\n",
    "x=imax2[0]\n",
    "print(list(df_soup.iloc[i]))\n",
    "print(list(df_soup.iloc[x]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jaccard score with weighted rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return recommended Podcast ID for input index of a review with a weighted rating\n",
    "# instead of including it within the jaccard score, the was multiplied by the jaccard score\n",
    "\n",
    "def recommend_podcast_jaccard_rating(df, index):\n",
    "    smax = 0\n",
    "    row = df.iloc[index]\n",
    "    for index2, row2 in df.iterrows():\n",
    "        #print(index, row['podcast_id'], row['soup'])\n",
    "    \n",
    "    #loop through column in those rows and get the jaccard score\n",
    "    #index != index2:\n",
    "        if row['podcast_id'] != row2['podcast_id']:\n",
    "            s = jaccard_similarity(row['soup'], row2['soup'])\n",
    "            \n",
    "            #weighted rating score\n",
    "            s = float(s) * float(row2['rating'])\n",
    "            \n",
    "            if s > smax:\n",
    "                smax = s\n",
    "                imax2 = index2\n",
    "    return imax2, smax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no real difference in results\n",
    "\n",
    "i = 1234\n",
    "imax2 = recommend_podcast_jaccard_rating(df_countmatrix,i)\n",
    "x=imax2[0]\n",
    "print(list(df_soup_r.iloc[i]))\n",
    "print(list(df_soup_r.iloc[x]))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4b6eb38bdf5e5ad4df4721abb2468d830c2ccff7ccbaee8a7326e2cd2ef9f9d4"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
